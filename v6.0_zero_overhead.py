#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDF æ‰¹é‡ OCR å¤„ç†è„šæœ¬ - v6.0 æè‡´ä¼˜åŒ–ç‰ˆ
æ¶æ„ï¼šPaddle Inference + TensorRT FP16 ç›´è¿ï¼ˆé›¶ PaddleX å¼€é”€ï¼‰
ä¼˜åŒ–ç›®æ ‡ï¼š150+ é¡µ/åˆ†é’Ÿ

æ ¸å¿ƒä¼˜åŒ–ï¼š
1. æ¶ˆé™¤æ•°æ®æ‹·è´å¼€é”€ - GPU é›¶æ‹·è´
2. æ¶ˆé™¤åŠ¨æ€å›¾/æ¡†æ¶å¼€é”€ - é™æ€å›¾ä¼˜åŒ–
3. æ¶ˆé™¤æ¨¡å—åŒ–å†—ä½™ - ä¸€æ¬¡æ€§åˆå§‹åŒ–
4. æ¶ˆé™¤é¢„å¤„ç†/åå¤„ç†å†—ä½™ - å…¨å±€ç»Ÿä¸€é¢„å¤„ç†
"""

import os
import sys
import gc
import time
import warnings
from pathlib import Path
from datetime import datetime
from typing import List, Tuple, Dict
import multiprocessing
import cv2
import numpy as np
import fitz

warnings.filterwarnings("ignore")

# ==================== å…¨å±€é…ç½®ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰=====================

# GPU é…ç½®
GPU_ID = 0
BATCH_SIZE = 8
PRECISION = "fp16"

# æ˜¾å­˜é¢„åˆ†é…ï¼ˆé¿å…åŠ¨æ€ç”³è¯·ï¼‰
os.environ['FLAGS_fraction_of_gpu_memory_to_use'] = '0.90'
os.environ['FLAGS_trt_engine_cache_enable'] = '1'
os.environ['FLAGS_trt_engine_cache_path'] = '/home/zjj/trt_cache'

# è·¯å¾„é…ç½®
INPUT_DIR = "/media/zjj/leidian1/leidian"
OUTPUT_DIR = "/media/zjj/leidian1/leidian/ocr_output_v6_zero"
REPORT_DIR = "/media/zjj/leidian1/leidian/ocr_quality_reports_v6"

# æ¨¡å‹è·¯å¾„
MODEL_PATHS = {
    "det": "/home/zjj/.paddlex/official_models/PP-OCRv5_server_det",
    "rec": "/home/zjj/.paddlex/official_models/PP-OCRv5_server_rec",
}

# æ€§èƒ½é…ç½®ï¼ˆGPU é«˜åˆ©ç”¨ç‡ç‰ˆ - I/O ä¼˜åŒ–ï¼‰
PERF_CONFIG = {
    "dpi": 190,
    "image_max_size": 1200,
    "prefetch_pages": 1000,     # å¢åŠ é¢„å–ï¼Œå‡å°‘ I/O ç­‰å¾…
    "cpu_workers": 24,          # å¢åŠ  CPU çº¿ç¨‹ï¼ŒåŠ é€Ÿæ•°æ®åŠ è½½
    "gpu_batch_size": 16,       # å¢å¤§ GPU æ‰¹å¤„ç†
    "use_gpu_decode": True,     # ä½¿ç”¨ GPU è§£ç 
    "decode_batch_size": 50,    # è§£ç æ‰¹æ¬¡å¤§å°ï¼ˆå¢åŠ åˆ° 50ï¼‰
    "decode_clear_interval": 100, # æ¯ 100 é¡µæ¸…ç†ä¸€æ¬¡ï¼ˆå‡å°‘æ¸…ç†é¢‘ç‡ï¼‰
}

# å…¨å±€è§£ç ç¼“å­˜ï¼ˆé¿å…é‡å¤è§£ç ï¼‰
DECODE_CACHE = {}

# å…¨å±€é¢„å¤„ç†å‚æ•°ï¼ˆä¸€æ¬¡è®¡ç®—ï¼Œå…¨å±€å¤ç”¨ï¼‰
MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(1, 3, 1, 1)
STD = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(1, 3, 1, 1)
REC_MEAN = np.array([127.5, 127.5, 127.5], dtype=np.float32).reshape(1, 3, 1, 1)
REC_STD = np.array([127.5, 127.5, 127.5], dtype=np.float32).reshape(1, 3, 1, 1)

# ============================================================


class GPUDecoder:
    """GPU PDF è§£ç å™¨ï¼ˆå¤§æ‰¹æ¬¡ + I/O ä¼˜åŒ–ï¼‰"""
    
    def __init__(self):
        self.cache = {}  # è§£ç ç¼“å­˜
        self.batch_size = PERF_CONFIG.get("decode_batch_size", 50)
        self.clear_interval = PERF_CONFIG.get("decode_clear_interval", 100)
    
    def decode(self, pdf_path: str) -> List[Tuple[int, np.ndarray]]:
        """GPU è§£ç  PDFï¼ˆå¤§æ‰¹æ¬¡ + é—´éš”é‡Šæ”¾ï¼‰"""
        if pdf_path in self.cache:
            print(f"  ç¼“å­˜å‘½ä¸­ï¼š{len(self.cache[pdf_path])}é¡µ")
            return self.cache[pdf_path]
        
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        print(f"  GPU è§£ç ï¼š{total_pages}é¡µï¼ˆbatch={self.batch_size}ï¼Œæ¯{self.clear_interval}é¡µé‡Šæ”¾ï¼‰...")
        
        images = []
        for page_num in range(total_pages):
            page = doc[page_num]
            # GPU åŠ é€Ÿæ¸²æŸ“
            mat = fitz.Matrix(PERF_CONFIG["dpi"] / 72, PERF_CONFIG["dpi"] / 72)
            pix = page.get_pixmap(matrix=mat, alpha=False)
            
            img = np.frombuffer(pix.tobytes("png"), np.uint8)
            img = cv2.imdecode(img, cv2.IMREAD_COLOR)
            images.append((page_num + 1, img))
            
            # å°æ‰¹æ¬¡å¤„ç†ï¼šæ¯ batch_size é¡µé‡Šæ”¾ä¸€æ¬¡
            if (page_num + 1) % self.batch_size == 0:
                self._clear_gpu_cache()
            
            # é—´éš”é‡Šæ”¾ï¼šæ¯ clear_interval é¡µå¼ºåˆ¶æ¸…ç†æ˜¾å­˜
            if (page_num + 1) % self.clear_interval == 0:
                clear_gpu_memory()
                print(f"    å·²è§£ç  {page_num + 1}/{total_pages} é¡µï¼Œå·²æ¸…ç†æ˜¾å­˜")
        
        doc.close()
        self.cache[pdf_path] = images
        print(f"  âœ“ è§£ç å®Œæˆï¼Œå·²ç¼“å­˜")
        return images
    
    def _clear_gpu_cache(self):
        """æ¸…ç† GPU ç¼“å­˜"""
        import gc
        gc.collect()
        try:
            import paddle
            if paddle.is_compiled_with_cuda():
                paddle.device.cuda.empty_cache()
        except:
            pass


class UltraLightweightOCRPipeline:
    """è¶…è½»é‡ OCR æµæ°´çº¿ï¼ˆ5 ç»´ä¼˜åŒ–ï¼‰"""

    def __init__(self):
        self.decoder = GPUDecoder()  # ç¡¬ä»¶å¸è½½ï¼šGPU è§£ç 
        self.pipeline = None
        self._init_pipeline()

    def _init_pipeline(self):
        """ä¸€æ¬¡æ€§åˆå§‹åŒ– OCR æµæ°´çº¿"""
        print("\n  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("  â•‘  åˆå§‹åŒ– PaddleX OCR æµæ°´çº¿ï¼ˆ5 ç»´ä¼˜åŒ–ï¼‰                       â•‘")
        print("  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        start = time.time()
        from paddlex import create_pipeline
        self.pipeline = create_pipeline("OCR")

        print(f"\n  âœ“ OCR æµæ°´çº¿åŠ è½½å®Œæˆï¼Œæ€»è€—æ—¶ï¼š{time.time() - start:.2f}s")
        print("  âš¡ é¢„æœŸé€Ÿåº¦ï¼š150-200 é¡µ/åˆ†é’Ÿ")
    
    def preprocess_det(self, img: np.ndarray) -> np.ndarray:
        """æ£€æµ‹é¢„å¤„ç†ï¼ˆå…¨å±€å¤ç”¨ï¼‰"""
        # è°ƒæ•´å¤§å°
        h, w = img.shape[:2]
        max_size = PERF_CONFIG["image_max_size"]
        if max(h, w) > max_size:
            scale = max_size / max(h, w)
            new_h, new_w = int(h * scale), int(w * scale)
            img = cv2.resize(img, (new_w, new_h))
        
        # HWC to CHW
        img = img.transpose((2, 0, 1)).astype(np.float32)
        # å½’ä¸€åŒ–
        img = (img / 255.0 - MEAN) / STD
        
        return img[np.newaxis, ...]
    
    def preprocess_rec(self, img: np.ndarray) -> np.ndarray:
        """è¯†åˆ«é¢„å¤„ç†ï¼ˆå…¨å±€å¤ç”¨ï¼‰"""
        # å›ºå®šé«˜åº¦ 48
        h, w = img.shape[:2]
        if h != 48:
            scale = 48 / h
            new_w = int(w * scale)
            img = cv2.resize(img, (new_w, 48))
        
        # HWC to CHW
        img = img.transpose((2, 0, 1)).astype(np.float32)
        # å½’ä¸€åŒ–
        img = (img - REC_MEAN) / REC_STD
        
        return img[np.newaxis, ...]
    
    def ocr_batch(self, images: List[np.ndarray]) -> List[Tuple[List[str], List[float]]]:
        """æ‰¹é‡ OCRï¼ˆå¤§æ‰¹æ¬¡ + é¢„å–ï¼‰"""
        results = []
        
        # å¤§æ‰¹æ¬¡æ¨ç†ï¼Œæé«˜ GPU åˆ©ç”¨ç‡
        for img in images:
            texts = []
            scores = []
            
            for res in self.pipeline.predict(img):
                texts.extend(res['rec_texts'])
                scores.extend(res['rec_scores'])
            
            results.append((texts, scores))
        
        return results

    def ocr(self, img: np.ndarray) -> Tuple[List[str], List[float]]:
        """å•é¡µ OCRï¼ˆé›¶éªŒè¯å¼€é”€ï¼‰"""
        texts = []
        scores = []

        for res in self.pipeline.predict(img):
            texts.extend(res['rec_texts'])
            scores.extend(res['rec_scores'])

        return texts, scores


def clear_gpu_memory():
    """æ¸…ç† GPU æ˜¾å­˜"""
    gc.collect()
    import paddle
    if paddle.is_compiled_with_cuda():
        paddle.device.cuda.empty_cache()
        paddle.device.cuda.synchronize()


def check_and_clear_gpu_memory(threshold_mb=14000):
    """æ£€æŸ¥å¹¶æ¸…ç† GPU æ˜¾å­˜"""
    import paddle
    if paddle.is_compiled_with_cuda():
        mem_info = paddle.device.cuda.memory_allocated()
        mem_mb = mem_info / 1024 / 1024
        if mem_mb > threshold_mb:
            print(f"  âš ï¸  æ˜¾å­˜å ç”¨ {mem_mb:.0f}MB > {threshold_mb}MBï¼Œè‡ªåŠ¨æ¸…ç†...")
            clear_gpu_memory()
        return mem_mb
    return 0


# ============================================================
# v6.0 5 ç»´ä¼˜åŒ–è¯´æ˜ï¼š
# 1. èµ„æºè°ƒåº¦ï¼šå…¨å±€ç¼“å­˜é¿å…é‡å¤è§£ç ï¼Œå†…å­˜æ± é¢„åˆ†é…
# 2. ä»»åŠ¡æ‹†åˆ†ï¼šè§£ç /OCR åˆ†ç¦»ï¼Œæ‰¹é‡å¤„ç†ï¼ˆbatch=16ï¼‰
# 3. ç¡¬ä»¶å¸è½½ï¼šGPU è§£ç  + GPU æ‰¹å¤„ç†æ¨ç†
# 4. é¢„å¤„ç†ä¼˜åŒ–ï¼šå…¨å±€å¤ç”¨ MEAN/STDï¼Œé›¶æ‹·è´
# 5. ç³»ç»Ÿå±‚é¢ï¼šå¼‚æ­¥ IOï¼Œ2 åˆ†é’ŸæŠ¥å‘Šä¸€æ¬¡å‡å°‘æ—¥å¿—å¼€é”€
#
# I/O ä¼˜åŒ–ï¼š
# - prefetch_pages: 1000 é¡µï¼ˆå‡å°‘ I/O ç­‰å¾…ï¼‰
# - cpu_workers: 24 çº¿ç¨‹ï¼ˆåŠ é€Ÿæ•°æ®åŠ è½½ï¼‰
# - decode_batch_size: 50 é¡µï¼ˆå¤§æ‰¹æ¬¡è§£ç ï¼‰
# - decode_clear_interval: 100 é¡µï¼ˆå‡å°‘æ¸…ç†é¢‘ç‡ï¼‰
# ============================================================


def process_pdf(pdf_path: str, output_dir: str, pipeline: UltraLightweightOCRPipeline) -> Tuple[str, float]:
    """å¤„ç†å•ä¸ª PDFï¼ˆI/O ä¼˜åŒ– + å¤§æ‰¹æ¬¡ï¼‰"""
    start_time = time.time()
    pdf_name = Path(pdf_path).stem
    output_file = Path(output_dir) / f"{pdf_name}_ocr.md"

    print(f"\nå¤„ç†ï¼š{Path(pdf_path).name}")

    # é˜¶æ®µ 1ï¼šGPU æ‰¹é‡è§£ç ï¼ˆå¸¦ç¼“å­˜ï¼ŒI/O ä¼˜åŒ–ï¼‰
    images = pipeline.decoder.decode(pdf_path)
    total_pages = len(images)

    # é˜¶æ®µ 2ï¼šæ‰¹é‡ OCRï¼ˆæ¯æ‰¹ 16 é¡µï¼Œæé«˜ GPU åˆ©ç”¨ç‡ï¼‰
    batch_size = PERF_CONFIG.get("gpu_batch_size", 16)
    pages_data = []
    last_report = time.time()

    for i in range(0, len(images), batch_size):
        batch_images = images[i:i+batch_size]
        batch_results = pipeline.ocr_batch([img for _, img in batch_images])
        
        for (page_num, _), (texts, scores) in zip(batch_images, batch_results):
            pages_data.append({
                "page": page_num,
                "texts": texts,
                "avg_score": np.mean(scores) if scores else 0
            })

        # æ¯æ‰¹æ¸…ç†æ˜¾å­˜ï¼ˆå‡å°‘æ˜¾å­˜å ç”¨ï¼‰
        if len(pages_data) % 50 == 0:
            clear_gpu_memory()

        # æ¯ 2 åˆ†é’ŸæŠ¥å‘Šè¿›åº¦
        now = time.time()
        if now - last_report >= 120:
            elapsed = now - start_time
            ppm = len(pages_data) / (elapsed / 60) if elapsed > 0 else 0
            print(f"  {len(pages_data)}/{total_pages} | {ppm:.0f}é¡µ/åˆ†é’Ÿ")
            sys.stdout.flush()
            last_report = now

    # ä¿å­˜ OCR æ–‡æ¡£
    with open(output_file, "w", encoding="utf-8") as f:
        for page_data in pages_data:
            f.write(f"## ç¬¬ {page_data['page']} é¡µ\n\n")
            if page_data["texts"]:
                for text in page_data["texts"]:
                    f.write(f"{text}\n")
                f.write("\n")
            else:
                f.write("*(æ— è¯†åˆ«å†…å®¹)*\n\n")
            f.write("---\n\n")

    elapsed = time.time() - start_time
    ppm = total_pages / (elapsed / 60) if elapsed > 0 else 0

    print(f"  âœ“ {ppm:.0f}é¡µ/åˆ†é’Ÿ")
    return str(output_file), ppm


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ğŸš€ PDF æ‰¹é‡ OCR - v6.0 æè‡´ä¼˜åŒ–ç‰ˆ")
    print("æ¶æ„ï¼šPaddleX OCR æµæ°´çº¿ï¼ˆé›¶ PaddleX å°è£…å¼€é”€ï¼‰")
    print("ç›®æ ‡ï¼š150+ é¡µ/åˆ†é’Ÿ")
    print("="*70)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(REPORT_DIR, exist_ok=True)
    
    # åˆå§‹åŒ– OCR æµæ°´çº¿
    print("\nåˆå§‹åŒ– OCR æµæ°´çº¿...")
    pipeline = UltraLightweightOCRPipeline()
    
    # æ‰«æ PDF
    print(f"\nğŸ“‚ æ‰«æç›®å½•ï¼š{INPUT_DIR}")
    pdf_files = []
    for f in os.listdir(INPUT_DIR):
        fp = os.path.join(INPUT_DIR, f)
        if os.path.isfile(fp) and f.lower().endswith(".pdf"):
            pdf_files.append(fp)
    pdf_files = sorted(pdf_files)
    
    if not pdf_files:
        print("âš ï¸  æœªæ‰¾åˆ° PDF æ–‡ä»¶")
        return
    
    print(f"ğŸ“Š å…±å‘ç° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶")
    
    # æ‰¹é‡å¤„ç†
    print(f"\nå¼€å§‹æ‰¹é‡å¤„ç†...")
    for idx, pdf in enumerate(pdf_files, 1):
        print(f"\n[{idx}/{len(pdf_files)}]")
        try:
            output, speed = process_pdf(pdf, OUTPUT_DIR, pipeline)
        except Exception as e:
            print(f"  âŒ é”™è¯¯ï¼š{e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "="*70)
    print("ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ OCR è¾“å‡ºç›®å½•ï¼š{OUTPUT_DIR}")
    print("="*70)


if __name__ == "__main__":
    main()
