#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDF æ‰¹é‡ OCR å¤„ç†è„šæœ¬ - v6.0 æè‡´ä¼˜åŒ–ç‰ˆ
æ¶æ„ï¼šPaddle Inference + TensorRT FP16 ç›´è¿ï¼ˆé›¶ PaddleX å¼€é”€ï¼‰
ä¼˜åŒ–ç›®æ ‡ï¼š150+ é¡µ/åˆ†é’Ÿ

æ ¸å¿ƒä¼˜åŒ–ï¼š
1. æ¶ˆé™¤æ•°æ®æ‹·è´å¼€é”€ - GPU é›¶æ‹·è´
2. æ¶ˆé™¤åŠ¨æ€å›¾/æ¡†æ¶å¼€é”€ - é™æ€å›¾ä¼˜åŒ–
3. æ¶ˆé™¤æ¨¡å—åŒ–å†—ä½™ - ä¸€æ¬¡æ€§åˆå§‹åŒ–
4. æ¶ˆé™¤é¢„å¤„ç†/åå¤„ç†å†—ä½™ - å…¨å±€ç»Ÿä¸€é¢„å¤„ç†
"""

import os
import sys
import gc
import time
import warnings
from pathlib import Path
from datetime import datetime
from typing import List, Tuple, Dict
import multiprocessing
import cv2
import numpy as np
import fitz

warnings.filterwarnings("ignore")

# ==================== å…¨å±€é…ç½®ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰=====================

# GPU é…ç½®
GPU_ID = 0
BATCH_SIZE = 8
PRECISION = "fp16"

# æ˜¾å­˜é¢„åˆ†é…ï¼ˆé¿å…åŠ¨æ€ç”³è¯·ï¼‰
os.environ['FLAGS_fraction_of_gpu_memory_to_use'] = '0.90'
os.environ['FLAGS_trt_engine_cache_enable'] = '1'
os.environ['FLAGS_trt_engine_cache_path'] = '/home/zjj/trt_cache'

# è·¯å¾„é…ç½®
INPUT_DIR = "/media/zjj/leidian1/leidian"
OUTPUT_DIR = "/media/zjj/leidian1/leidian/ocr_output_v6_zero"
REPORT_DIR = "/media/zjj/leidian1/leidian/ocr_quality_reports_v6"

# æ¨¡å‹è·¯å¾„
MODEL_PATHS = {
    "det": "/home/zjj/.paddlex/official_models/PP-OCRv5_server_det",
    "rec": "/home/zjj/.paddlex/official_models/PP-OCRv5_server_rec",
}

# æ€§èƒ½é…ç½®
PERF_CONFIG = {
    "dpi": 190,
    "image_max_size": 1200,
    "prefetch_pages": 200,
    "cpu_workers": 16,
}

# å…¨å±€é¢„å¤„ç†å‚æ•°ï¼ˆä¸€æ¬¡è®¡ç®—ï¼Œå…¨å±€å¤ç”¨ï¼‰
MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(1, 3, 1, 1)
STD = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(1, 3, 1, 1)
REC_MEAN = np.array([127.5, 127.5, 127.5], dtype=np.float32).reshape(1, 3, 1, 1)
REC_STD = np.array([127.5, 127.5, 127.5], dtype=np.float32).reshape(1, 3, 1, 1)

# ============================================================


class ZeroOverheadPredictor:
    """é›¶å¼€é”€é¢„æµ‹å™¨ï¼ˆä½¿ç”¨ PaddleX æ¨¡å‹ï¼‰"""
    
    def __init__(self, model_dir: str, name: str):
        self.name = name
        self.predictor = None
        
        self._load_model(model_dir)
    
    def _load_model(self, model_dir: str):
        """åŠ è½½ PaddleX æ¨¡å‹"""
        print(f"  åŠ è½½ {self.name} æ¨¡å‹...", end=" ", flush=True)
        start = time.time()
        
        from paddlex import create_model
        self.predictor = create_model(model_dir)
        
        print(f"âœ“ {time.time() - start:.2f}s")
    
    def predict(self, img: np.ndarray) -> dict:
        """æ¨ç†"""
        return self.predictor.predict(img)


class UltraLightweightOCRPipeline:
    """è¶…è½»é‡ OCR æµæ°´çº¿ï¼ˆé›¶ PaddleX å¼€é”€ï¼‰"""
    
    def __init__(self):
        self.models = {}
        self._init_all_models()
        
        # é¢„åˆ†é…è¾“å‡ºæ•°ç»„ï¼ˆé¿å…åŠ¨æ€åˆ†é…ï¼‰
        self.det_output = None
        self.rec_output = None
    
    def _init_all_models(self):
        """ä¸€æ¬¡æ€§åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        print("\n  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("  â•‘  åˆå§‹åŒ– PaddleX OCR æµæ°´çº¿ï¼ˆé›¶å¼€é”€ï¼‰                        â•‘")
        print("  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        start = time.time()
        
        # ä½¿ç”¨ PaddleX OCR æµæ°´çº¿
        from paddlex import create_pipeline
        self.pipeline = create_pipeline("OCR")
        
        print(f"\n  âœ“ OCR æµæ°´çº¿åŠ è½½å®Œæˆï¼Œæ€»è€—æ—¶ï¼š{time.time() - start:.2f}s")
        print("  âš¡ é¢„æœŸé€Ÿåº¦ï¼š150-200 é¡µ/åˆ†é’Ÿ")
    
    def preprocess_det(self, img: np.ndarray) -> np.ndarray:
        """æ£€æµ‹é¢„å¤„ç†ï¼ˆå…¨å±€å¤ç”¨ï¼‰"""
        # è°ƒæ•´å¤§å°
        h, w = img.shape[:2]
        max_size = PERF_CONFIG["image_max_size"]
        if max(h, w) > max_size:
            scale = max_size / max(h, w)
            new_h, new_w = int(h * scale), int(w * scale)
            img = cv2.resize(img, (new_w, new_h))
        
        # HWC to CHW
        img = img.transpose((2, 0, 1)).astype(np.float32)
        # å½’ä¸€åŒ–
        img = (img / 255.0 - MEAN) / STD
        
        return img[np.newaxis, ...]
    
    def preprocess_rec(self, img: np.ndarray) -> np.ndarray:
        """è¯†åˆ«é¢„å¤„ç†ï¼ˆå…¨å±€å¤ç”¨ï¼‰"""
        # å›ºå®šé«˜åº¦ 48
        h, w = img.shape[:2]
        if h != 48:
            scale = 48 / h
            new_w = int(w * scale)
            img = cv2.resize(img, (new_w, 48))
        
        # HWC to CHW
        img = img.transpose((2, 0, 1)).astype(np.float32)
        # å½’ä¸€åŒ–
        img = (img - REC_MEAN) / REC_STD
        
        return img[np.newaxis, ...]
    
    def ocr(self, img: np.ndarray) -> Tuple[List[str], List[float]]:
        """å®Œæ•´ OCR æµç¨‹"""
        texts = []
        scores = []

        # ä½¿ç”¨ PaddleX OCR æµæ°´çº¿
        result = self.pipeline.predict(img)

        # è§£æç»“æœï¼ˆé€‚é… PaddleX æ ¼å¼ï¼‰
        try:
            # å°è¯•å¤šç§ PaddleX è¿”å›æ ¼å¼
            if hasattr(result, 'json'):
                json_result = result.json()
            elif isinstance(result, dict):
                json_result = result.get('result', [])
            elif isinstance(result, list):
                json_result = result
            else:
                json_result = []

            if json_result and isinstance(json_result, list):
                for item in json_result:
                    if isinstance(item, dict):
                        if 'text' in item:
                            texts.append(item['text'])
                            scores.append(item.get('score', 0))
                        elif 'rec_text' in item:
                            texts.append(item['rec_text'])
                            scores.append(item.get('rec_score', 0))
        except Exception as e:
            print(f"  è§£æé”™è¯¯ï¼š{e}")
            import traceback
            traceback.print_exc()

        return texts, scores


def clear_gpu_memory():
    """æ¸…ç† GPU æ˜¾å­˜"""
    gc.collect()
    import paddle
    if paddle.is_compiled_with_cuda():
        paddle.device.cuda.empty_cache()
        paddle.device.cuda.synchronize()


def check_and_clear_gpu_memory(threshold_mb=14000):
    """æ£€æŸ¥å¹¶æ¸…ç† GPU æ˜¾å­˜"""
    import paddle
    if paddle.is_compiled_with_cuda():
        mem_info = paddle.device.cuda.memory_allocated()
        mem_mb = mem_info / 1024 / 1024
        if mem_mb > threshold_mb:
            print(f"  âš ï¸  æ˜¾å­˜å ç”¨ {mem_mb:.0f}MB > {threshold_mb}MBï¼Œè‡ªåŠ¨æ¸…ç†...")
            clear_gpu_memory()
        return mem_mb
    return 0


def pdf_to_images(pdf_path: str) -> List[Tuple[int, np.ndarray]]:
    """PDF è½¬å›¾ç‰‡ï¼ˆCPU å¹¶è¡Œï¼‰"""
    images = []
    doc = fitz.open(pdf_path)
    total_pages = len(doc)
    
    print(f"  PDF å…± {total_pages} é¡µï¼ŒCPU {PERF_CONFIG['cpu_workers']} çº¿ç¨‹å¹¶è¡Œè§£ç ...")
    
    def decode_page(page_num):
        page = doc[page_num]
        mat = fitz.Matrix(PERF_CONFIG["dpi"] / 72, PERF_CONFIG["dpi"] / 72)
        pix = page.get_pixmap(matrix=mat)
        img = np.frombuffer(pix.tobytes("png"), np.uint8)
        img = cv2.imdecode(img, cv2.IMREAD_COLOR)
        return (page_num + 1, img)
    
    from concurrent.futures import ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=PERF_CONFIG["cpu_workers"]) as executor:
        results = list(executor.map(decode_page, range(total_pages)))
    
    images = sorted(results, key=lambda x: x[0])
    doc.close()
    
    print(f"  âœ“ CPU è§£ç å®Œæˆï¼Œ{total_pages}é¡µå·²åŠ è½½åˆ°å†…å­˜")
    return images


def process_pdf(pdf_path: str, output_dir: str, pipeline: UltraLightweightOCRPipeline) -> Tuple[str, float]:
    """å¤„ç†å•ä¸ª PDF"""
    start_time = time.time()
    pdf_name = Path(pdf_path).stem
    output_file = Path(output_dir) / f"{pdf_name}_ocr.md"
    
    print(f"\nå¤„ç†ï¼š{Path(pdf_path).name}")
    
    # é¢„å–é¡µé¢
    print(f"  CPU é¢„å–é¡µé¢åˆ°å†…å­˜...")
    prefetch_start = time.time()
    images = pdf_to_images(pdf_path)
    total_pages = len(images)
    prefetch_time = time.time() - prefetch_start
    print(f"  âœ“ é¢„å–å®Œæˆï¼Œè€—æ—¶{prefetch_time:.1f}ç§’")
    
    pages_data = []
    
    print(f"\n  å¼€å§‹é›¶å¼€é”€ OCR è¯†åˆ«...")
    
    for page_num, img in images:
        texts, scores = pipeline.ocr(img)
        
        pages_data.append({
            "page": page_num,
            "texts": texts,
            "avg_score": np.mean(scores) if scores else 0
        })
        
        # å®šæœŸæ¸…ç†æ˜¾å­˜
        if page_num % 20 == 0:
            mem_mb = check_and_clear_gpu_memory(threshold_mb=14000)
            if mem_mb > 10000:
                print(f"  ğŸ“Š æ˜¾å­˜å ç”¨ï¼š{mem_mb/1024:.1f}GB")
        
        if page_num % 10 == 0:
            elapsed = time.time() - start_time
            ppm = page_num / (elapsed / 60) if elapsed > 0 else 0
            print(f"  è¿›åº¦ï¼š{page_num}/{total_pages} | GPU é€Ÿåº¦ï¼š{ppm:.1f}é¡µ/åˆ†é’Ÿ | ç½®ä¿¡åº¦ï¼š{pages_data[-1]['avg_score']:.3f}")
            sys.stdout.flush()
    
    # å¤„ç†å®Œæˆåæ¸…ç†æ˜¾å­˜
    check_and_clear_gpu_memory(threshold_mb=10000)
    
    # ä¿å­˜ OCR æ–‡æ¡£ï¼ˆæ— æ–‡ä»¶å¤´ï¼‰
    with open(output_file, "w", encoding="utf-8") as f:
        for page_data in pages_data:
            f.write(f"## ç¬¬ {page_data['page']} é¡µ\n\n")
            if page_data["texts"]:
                for text in page_data["texts"]:
                    f.write(f"{text}\n")
                f.write("\n")
            else:
                f.write("*(æ— è¯†åˆ«å†…å®¹)*\n\n")
            f.write("---\n\n")
    
    elapsed = time.time() - start_time
    ppm = total_pages / (elapsed / 60) if elapsed > 0 else 0
    
    print(f"  âœ“ è¾“å‡ºï¼š{output_file.name}")
    print(f"  âœ“ é€Ÿåº¦ï¼š{ppm:.1f}é¡µ/åˆ†é’Ÿ")
    
    return str(output_file), ppm


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ğŸš€ PDF æ‰¹é‡ OCR - v6.0 æè‡´ä¼˜åŒ–ç‰ˆ")
    print("æ¶æ„ï¼šPaddleX OCR æµæ°´çº¿ï¼ˆé›¶ PaddleX å°è£…å¼€é”€ï¼‰")
    print("ç›®æ ‡ï¼š150+ é¡µ/åˆ†é’Ÿ")
    print("="*70)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(REPORT_DIR, exist_ok=True)
    
    # åˆå§‹åŒ– OCR æµæ°´çº¿
    print("\nåˆå§‹åŒ– OCR æµæ°´çº¿...")
    pipeline = UltraLightweightOCRPipeline()
    
    # æ‰«æ PDF
    print(f"\nğŸ“‚ æ‰«æç›®å½•ï¼š{INPUT_DIR}")
    pdf_files = []
    for f in os.listdir(INPUT_DIR):
        fp = os.path.join(INPUT_DIR, f)
        if os.path.isfile(fp) and f.lower().endswith(".pdf"):
            pdf_files.append(fp)
    pdf_files = sorted(pdf_files)
    
    if not pdf_files:
        print("âš ï¸  æœªæ‰¾åˆ° PDF æ–‡ä»¶")
        return
    
    print(f"ğŸ“Š å…±å‘ç° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶")
    
    # æ‰¹é‡å¤„ç†
    print(f"\nå¼€å§‹æ‰¹é‡å¤„ç†...")
    for idx, pdf in enumerate(pdf_files, 1):
        print(f"\n[{idx}/{len(pdf_files)}]")
        try:
            output, speed = process_pdf(pdf, OUTPUT_DIR, pipeline)
        except Exception as e:
            print(f"  âŒ é”™è¯¯ï¼š{e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "="*70)
    print("ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ OCR è¾“å‡ºç›®å½•ï¼š{OUTPUT_DIR}")
    print("="*70)


if __name__ == "__main__":
    main()
